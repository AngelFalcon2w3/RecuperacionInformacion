{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f8b5c16e7eb563",
   "metadata": {},
   "source": [
    "# Ejercicio 6: Introducción a Dense Retrieval\n",
    "\n",
    "## Objetivo de la práctica\n",
    "\n",
    "Generar embeddings con sentence-transformers (SBERT, E5), y recuperarlos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd69ed7fcbeef9d",
   "metadata": {},
   "source": [
    "## Parte 0: Carga del Corpus\n",
    "### Actividad\n",
    "\n",
    "1. Carga el corpus 20 Newsgroups desde sklearn.datasets.fetch_20newsgroups.\n",
    "2. Limita el corpus a los primeros 2000 documentos para facilitar el procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00fbde6cfc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroupsdocs = newsgroups.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174a1308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset newsgroupsdocs tiene 18846 documentos \n"
     ]
    }
   ],
   "source": [
    "num_documentos = len(newsgroupsdocs)\n",
    "print(f\"El dataset newsgroupsdocs tiene {num_documentos} documentos \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458ba25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitación corpus a los  primeros 2000 documentos\n",
    "newsgroupsdocs = newsgroupsdocs[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd202c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contenido documento 0 \n",
    "print(newsgroupsdocs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9184f4b3e66e20a",
   "metadata": {},
   "source": [
    "## Parte 2: Generación de Embeddings\n",
    "### Actividad\n",
    "\n",
    "1. Usa dos modelos de sentence-transformers. Puedes usar: `'all-MiniLM-L6-v2'` (SBERT), o `'intfloat/e5-base'` (E5). Cuando uses E5, antepon `\"passage: \"` a cada documento antes de codificar.\n",
    "2. Genera los vectores de embeddings para todos los documentos usando el modelo seleccionado.\n",
    "3. Guarda los embeddings en un array de NumPy para su posterior indexación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525ae7515c6169d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Batches: 100%|██████████| 63/63 [00:48<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de los embeddings: (2000, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Modelo SBERT\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Generamos embeddings para todos los documentos\n",
    "embeddings = model.encode(newsgroupsdocs, show_progress_bar=True)\n",
    "\n",
    "# Guardarmos como array NumPy\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "print(\"Shape de los embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40462a067ca2d379",
   "metadata": {},
   "source": [
    "## Parte 3: Consulta\n",
    "### Actividad\n",
    "\n",
    "1. Escribe una consulta en lenguaje natural. Ejemplos:\n",
    "\n",
    "    * \"God, religion, and spirituality\"\n",
    "    * \"space exploration\"\n",
    "    * \"car maintenance\"\n",
    "\n",
    "2. Codifica la consulta utilizando el mismo modelo de embeddings. Cuando uses E5, antepon `\"query: \"` a la consulta.\n",
    "3. Recupera los 5 documentos más relevantes con similitud coseno.\n",
    "4. Muestra los textos de los documentos recuperados (puedes mostrar solo los primeros 500 caracteres de cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aad085806124c709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta: space exploration\n",
      "\n",
      "Top 5 documentos más relevantes para: 'space exploration'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1. Documento 495 (Similitud: 0.4991)\n",
      "I am posting this for a friend without internet access. Please inquire\n",
      "to the phone number and address listed.\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\"Space: Teaching's Newest Frontier\"\n",
      "Sponsored by the Planetary Studies Foundation\n",
      "\n",
      "The Planetary Studies Foundation is sponsoring a one week class for\n",
      "teachers called \"Space: Teaching's Newest Frontier.\" The class will be\n",
      "held at the Sheraton Suites in Elk Grove, Illinois from June 14 through\n",
      "June 18. Participants wh\n",
      "================================================================================\n",
      "\n",
      "2. Documento 1643 (Similitud: 0.4398)\n",
      "\n",
      "Well, here goes.\n",
      "\n",
      "The first item of business is to establish the importance space life\n",
      "sciences in the whole of scheme of humankind.  I mean compared\n",
      "to football and baseball, the average joe schmoe doesn't seem interested\n",
      "or even curious about spaceflight.  I think that this forum can\n",
      "make a major change in that lack of insight and education.\n",
      "\n",
      "All of us, in our own way, can contribute to a comprehensive document\n",
      "which can be released to the general public around the world.  The\n",
      "document would \n",
      "================================================================================\n",
      "\n",
      "3. Documento 786 (Similitud: 0.4321)\n",
      "Ron Miller is a space artist with a long and distinguished career.  \n",
      "I've admired both his paintings (remember the USPS Solar System\n",
      "Exploration Stamps last year?) and his writings on the history of\n",
      "spaceflight.  For several years he's been working on a *big* project\n",
      "which is almost ready to hit the streets.  A brochure from his\n",
      "publisher has landed in my mailbox, and I thought it was cool enough\n",
      "to type in part of it (it's rather long).  Especially given the Net's\n",
      "strong interest in vaporware s\n",
      "================================================================================\n",
      "\n",
      "4. Documento 1199 (Similitud: 0.3995)\n",
      "Any comments on the absorbtion of the Office of Exploration into the\n",
      "Office of Space Sciences and the reassignment of Griffin to the \"Chief\n",
      "Engineer\" position?  Is this just a meaningless administrative\n",
      "shuffle, or does this bode ill for SEI?\n",
      "\n",
      "In my opinion, this seems like a Bad Thing, at least on the surface.\n",
      "Griffin seemed to be someone who was actually interested in getting\n",
      "things done, and who was willing to look an innovative approaches to\n",
      "getting things done faster, better, and cheaper.  \n",
      "================================================================================\n",
      "\n",
      "5. Documento 25 (Similitud: 0.3746)\n",
      "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Consulta \n",
    "query = \"space exploration\"\n",
    "print(f\"Consulta: {query}\")\n",
    " \n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "\n",
    "# Recuperamos los 5 documentos más relevantes usando similitud coseno\n",
    "similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "\n",
    "# Obtenemos los índices de los 5 documentos más similares\n",
    "top_5 = similarities.argsort()[-5:][::-1]\n",
    "\n",
    "# Resultado\n",
    "print(f\"\\nTop 5 documentos más relevantes para: '{query}'\\n\")\n",
    "for i, idx in enumerate(top_5, 1):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{i}. Documento {idx} (Similitud: {similarities[idx]:.4f})\")\n",
    "    print(newsgroupsdocs[idx][:500])  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9e5e7815c7508",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
